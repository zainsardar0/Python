{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gini_impurity' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 112\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# Initialize and train the random forest model\u001b[39;00m\n\u001b[0;32m    111\u001b[0m rf_model \u001b[38;5;241m=\u001b[39m RandomForest(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m--> 112\u001b[0m \u001b[43mrf_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[0;32m    115\u001b[0m predictions \u001b[38;5;241m=\u001b[39m rf_model\u001b[38;5;241m.\u001b[39mpredict(X)\n",
      "Cell \u001b[1;32mIn[5], line 92\u001b[0m, in \u001b[0;36mRandomForest.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrees \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators):\n\u001b[1;32m---> 92\u001b[0m     tree \u001b[38;5;241m=\u001b[39m \u001b[43mDecisionTree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_samples_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_samples_split\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m     X_sample, y_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bootstrap_sample(X, y)\n\u001b[0;32m     94\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X_sample, y_sample)\n",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m, in \u001b[0;36mDecisionTree.__init__\u001b[1;34m(self, max_depth, min_samples_split, criterion)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_depth \u001b[38;5;241m=\u001b[39m max_depth\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_samples_split \u001b[38;5;241m=\u001b[39m min_samples_split\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion \u001b[38;5;241m=\u001b[39m \u001b[43mgini_impurity\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m criterion \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgini\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m entropy\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gini_impurity' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2, criterion='gini'):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.criterion = gini_impurity if criterion == 'gini' else entropy\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.root = self._grow_tree(X, y)\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        num_samples, num_features = X.shape\n",
    "        if num_samples >= self.min_samples_split and (self.max_depth is None or depth < self.max_depth):\n",
    "            best_feature, best_threshold = self._best_split(X, y)\n",
    "            if best_feature is not None:\n",
    "                left_idxs, right_idxs = self._split(X[:, best_feature], best_threshold)\n",
    "                left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n",
    "                right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n",
    "                return Node(feature=best_feature, threshold=best_threshold, left=left, right=right)\n",
    "        return Node(value=most_common_label(y))\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        if num_samples <= 1:\n",
    "            return None, None\n",
    "\n",
    "        best_gain = -1\n",
    "        best_feature, best_threshold = None, None\n",
    "\n",
    "        for feature in range(num_features):\n",
    "            thresholds, classes = zip(*sorted(zip(X[:, feature], y)))\n",
    "            num_left = [0] * len(set(y))\n",
    "            num_right = np.bincount(classes)\n",
    "\n",
    "            for i in range(1, num_samples):\n",
    "                c = classes[i - 1]\n",
    "                num_left[c] += 1\n",
    "                num_right[c] -= 1\n",
    "\n",
    "                gain = self._information_gain(y, num_left, num_right, i)\n",
    "\n",
    "                if thresholds[i] == thresholds[i - 1]:\n",
    "                    continue\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature\n",
    "                    best_threshold = (thresholds[i] + thresholds[i - 1]) / 2\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def _information_gain(self, y, num_left, num_right, split_idx):\n",
    "        num = len(y)\n",
    "        p_left = split_idx / num\n",
    "        p_right = 1 - p_left\n",
    "        impurity = self.criterion(y)\n",
    "        left_impurity = self.criterion(num_left)\n",
    "        right_impurity = self.criterion(num_right)\n",
    "        return impurity - (p_left * left_impurity + p_right * right_impurity)\n",
    "\n",
    "    def _split(self, X_column, split_threshold):\n",
    "        left_idxs = np.argwhere(X_column <= split_threshold).flatten()\n",
    "        right_idxs = np.argwhere(X_column > split_threshold).flatten()\n",
    "        return left_idxs, right_idxs\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict(inputs) for inputs in X])\n",
    "\n",
    "    def _predict(self, inputs):\n",
    "        node = self.root\n",
    "        while node.value is None:\n",
    "            if inputs[node.feature] <= node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node.value\n",
    "import numpy as np\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, n_estimators=100, max_depth=None, min_samples_split=2, criterion='gini'):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.criterion = criterion\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.trees = []\n",
    "        for _ in range(self.n_estimators):\n",
    "            tree = DecisionTree(max_depth=self.max_depth, min_samples_split=self.min_samples_split, criterion=self.criterion)\n",
    "            X_sample, y_sample = self._bootstrap_sample(X, y)\n",
    "            tree.fit(X_sample, y_sample)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def _bootstrap_sample(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        return X[indices], y[indices]\n",
    "\n",
    "    def predict(self, X):\n",
    "        tree_predictions = np.array([tree.predict(X) for tree in self.trees])\n",
    "        return np.swapaxes(tree_predictions, 0, 1).mean(axis=1).round().astype(int)\n",
    "    # Generate some example data\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 2)\n",
    "y = (X[:, 0] + X[:, 1] > 1).astype(int)\n",
    "\n",
    "# Initialize and train the random forest model\n",
    "rf_model = RandomForest(n_estimators=10, max_depth=3)\n",
    "rf_model.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "predictions = rf_model.predict(X)\n",
    "\n",
    "# Print the accuracy\n",
    "accuracy = np.mean(predictions == y)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
